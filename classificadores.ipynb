{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto - Mineração de Texto e Web\n",
    "## Residência Engenharia e Ciência de dados - Samsung/UFPE\n",
    "\n",
    "### Lucas Couri - lncc2\n",
    "### Mariama Oliveira - mcso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv2D, Input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews_v2.csv\")\n",
    "df = df[df[\"reviews\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento (com e sem stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "other_punctuation = '—“”'  \n",
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.append('’')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "\n",
    "#Function that removes punctuation \n",
    "def remove_punctuation(text):\n",
    "    punctuation_free_doc = \"\".join([i for i in text if i not in string.punctuation+other_punctuation])\n",
    "    return punctuation_free_doc\n",
    "\n",
    "\n",
    "def remove_stopwords(list_words):\n",
    "    filtered_words = [word for word in list_words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def do_stemming(list_words):\n",
    "    stem_text = [stemmer.stem(word) for word in list_words]\n",
    "    return stem_text\n",
    "\n",
    "\n",
    "def pre_process(doc, basic_processing = False, no_stopwords = False, stemming = False):\n",
    "\n",
    "    final_doc = doc\n",
    "    \n",
    "    ## print(final_doc)\n",
    "\n",
    "    if basic_processing == True:\n",
    "        \n",
    "        final_doc = remove_punctuation(doc)\n",
    "        final_doc = final_doc.lower()\n",
    "\n",
    "    final_doc = nltk.word_tokenize(final_doc)\n",
    "\n",
    "    if no_stopwords == True:\n",
    "        final_doc = remove_stopwords(final_doc)    \n",
    "\n",
    "    if stemming == True:\n",
    "        final_doc = do_stemming(final_doc)\n",
    "\n",
    "    return final_doc\n",
    "\n",
    "def pre_process_all(df, pre_processing_list):\n",
    "\n",
    "    for param, index in zip(pre_processing_list, range(len(pre_processing_list))):\n",
    "        \n",
    "        df[f\"reviews_pipeline_{index}\"] = df[\"reviews\"].apply(lambda x: pre_process(x, **param))\n",
    "\n",
    "    return df\n",
    "\n",
    "pre_processing_list = [\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": False},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": True}]\n",
    "\n",
    "df_pp = pre_process_all(df, pre_processing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[[\"reviews_pipeline_0\", \"reviews_pipeline_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[\"class\"] = df_pp[\"stars\"].apply(lambda x : 1 if x >=4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that oversaples given a dataframe\n",
    "def perform_oversample(df):\n",
    "\n",
    "    class_1,class_0 = df[\"class\"].value_counts()\n",
    "    c0 = df[df['class'] == 0]\n",
    "    c1 = df[df['class'] == 1]\n",
    "\n",
    "    df_0 = c0.sample(round(class_1/3), replace=True)\n",
    "    oversampled_df = pd.concat([c1,df_0], axis=0)\n",
    "\n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define if it will perform oversampling\n",
    "OVERSAMPLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pp[[\"reviews_pipeline_0\", \"reviews_pipeline_1\", \"class\"]]\n",
    "y = df_pp[\"class\"]\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.3, random_state = 42, stratify=y_train_valid)\n",
    "\n",
    "if OVERSAMPLE:\n",
    "    X_train = perform_oversample(X_train)\n",
    "    \n",
    "y_train =  X_train[\"class\"]    \n",
    "X_train = X_train[[\"reviews_pipeline_0\", \"reviews_pipeline_1\"]]\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando tokens em string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_join = []\n",
    "X_train_join.append(X_train[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_train_join.append(X_train[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_train_join[0] = X_train_join[0].to_numpy()\n",
    "X_train_join[1] = X_train_join[1].to_numpy()\n",
    "\n",
    "X_test_join = []\n",
    "X_test_join.append(X_test[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_test_join.append(X_test[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_test_join[0] = X_test_join[0].to_numpy()\n",
    "X_test_join[1] = X_test_join[1].to_numpy()\n",
    "\n",
    "X_valid_join = []\n",
    "X_valid_join.append(X_valid[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_valid_join.append(X_valid[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_valid_join[0] = X_valid_join[0].to_numpy()\n",
    "X_valid_join[1] = X_valid_join[1].to_numpy()\n",
    "\n",
    "X_train_valid_join = []\n",
    "X_train_valid_join.append(X_train_valid[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_train_valid_join.append(X_train_valid[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_train_valid_join[0] = X_train_valid_join[0].to_numpy()\n",
    "X_train_valid_join[1] = X_train_valid_join[1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest com BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "X_train_vec = []\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[0]))\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[1]))\n",
    "\n",
    "X_test_vec = []\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[0]))\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[1]))\n",
    "\n",
    "# X_valid_vec = []\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[0]))\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[1]))\n",
    "\n",
    "\n",
    "print(X_train_vec[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "def objective(trial):\n",
    "      iris = sklearn.datasets.load_iris()\n",
    "      n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
    "      max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))\n",
    "      clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "      return sklearn.model_selection.cross_val_score(clf, iris.data, iris.target, \n",
    "           n_jobs=-1, cv=3).mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_rf(X_train, y_train, parameters, cv, SEED):\n",
    "\n",
    "    rf = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "    search = GridSearchCV(rf,\n",
    "                          parameters,\n",
    "                          scoring = \"accuracy\",\n",
    "                          n_jobs = -1,\n",
    "                          cv = cv)\n",
    "\n",
    "    result_rf = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Grid Search para Random Forest==========')\n",
    "    print(f'Melhor Score: {result_rf.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_rf.best_params_}')\n",
    "\n",
    "    return result_rf\n",
    "\n",
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_0 = val_rf(X_train_vec[0], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_0.best_params_) \n",
    "forest = forest.fit(X_train_vec[0], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[0]) \n",
    "result = forest.predict(X_test_vec[0])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_1 = val_rf(X_train_vec[1], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_1.best_params_) \n",
    "forest = forest.fit(X_train_vec[1], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[1]) \n",
    "result = forest.predict(X_test_vec[1])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes (CNN, LSTM e BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=None,\n",
    "#     standardize='lower_and_strip_punctuation',\n",
    "#     split='whitespace',\n",
    "#     ngrams=None,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=None,\n",
    "#     pad_to_max_tokens=False,\n",
    "#     vocabulary=None,\n",
    "#     idf_weights=None,\n",
    "#     sparse=False,\n",
    "#     ragged=False,\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_join[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Sem stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder_0 = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                            standardize=None\n",
    "                                            )\n",
    "#encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "encoder_0.adapt(X_train_join[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = encoder_0(X_train_join[0])\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder_0.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Com stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder_1 = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                            standardize=None\n",
    "                                            )\n",
    "#encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "encoder_1.adapt(X_train_join[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = encoder_1(X_train_join[1])\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder_1.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "##Testando keras tuner\n",
    "\n",
    "def model_builder_0(hp):\n",
    "\n",
    "    # hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    hp_rate = hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.05)\n",
    "\n",
    "    model_CNN_0 = tf.keras.Sequential([\n",
    "    encoder_0,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_0.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(rate=hp_rate),\n",
    "    # tf.keras.layers.Dense(units=hp_units, activation='relu'), #>>>>>Hiperparametro\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model_CNN_0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), #>>>>>Hiperparametro\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_CNN_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_0 = tf.keras.Sequential([\n",
    "#     encoder_0,\n",
    "#     tf.keras.layers.Embedding(\n",
    "#         input_dim=len(encoder_0.get_vocabulary()),\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True),\n",
    "#     tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "#     tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "#     #tf.keras.layers.Flatten(),    \n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.GlobalMaxPool1D(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_CNN_0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_CNN_0.fit(X_train_join[0], y_train, epochs=30,\n",
    "#                     batch_size = 32,\n",
    "#                     validation_data= (X_valid_join[0], y_valid),\n",
    "#                     validation_steps=30\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder_0, # the hypermodel\n",
    "                     objective='val_accuracy', # objective to optimize\n",
    "max_epochs=50,\n",
    "factor=3, # factor which you have seen above \n",
    "directory='tuner', # directory to save logs \n",
    "project_name='cnn_0_bal_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning settings\n",
    "tuner.search_space_summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Perform hypertuning\n",
    "tuner.search(X_train_join[1], y_train, epochs=50, validation_data = (X_valid_join[0], y_valid), callbacks=[stop_early])\n",
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "model_CNN_0 = tuner.hypermodel.build(best_hp)\n",
    "model_CNN_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_CNN_0.fit(X_train_join[0], y_train, epochs=50, validation_data = (X_valid_join[0], y_valid), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_CNN_0.evaluate(X_test_join[0], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_CNN_0.predict(X_test_join[0])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_1 = tf.keras.Sequential([\n",
    "#     encoder_1,\n",
    "#     tf.keras.layers.Embedding(\n",
    "#         input_dim=len(encoder_1.get_vocabulary()),\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True),\n",
    "#     tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "#     tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "#     #tf.keras.layers.Flatten(),    \n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.GlobalMaxPool1D(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "##Testando keras tuner\n",
    "\n",
    "def model_builder_1(hp):\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    model_CNN_1 = tf.keras.Sequential([\n",
    "    encoder_1,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_1.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    #tf.keras.layers.Flatten(),    \n",
    "    tf.keras.layers.Dense(units=hp_units, activation='relu'), #>>>>>Hiperparametro\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model_CNN_1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), #>>>>>Hiperparametro\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder_1, # the hypermodel\n",
    "                     objective='val_accuracy', # objective to optimize\n",
    "max_epochs=10,\n",
    "factor=3, # factor which you have seen above \n",
    "directory='tuner', # directory to save logs \n",
    "project_name='cnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning settings\n",
    "tuner.search_space_summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Perform hypertuning\n",
    "tuner.search(X_train_join[1], y_train, epochs=10, validation_data = (X_valid_join[1], y_valid), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "model_CNN_1 = tuner.hypermodel.build(best_hp)\n",
    "model_CNN_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_CNN_1.fit(X_train_join[1], y_train, epochs=10, validation_data = (X_valid_join[1], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_CNN_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_CNN_1.fit(X_train_join[1], y_train, epochs=20,\n",
    "#                     batch_size = 32,\n",
    "#                     validation_data= (X_valid_join[1], y_valid),\n",
    "#                     validation_steps=3\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model_CNN_1.evaluate(X_test_join[1], y_test)\n",
    "test_loss, test_acc = model_CNN_1.evaluate(X_test_join[1], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = h_model.predict(X_test_join[1])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM0 = tf.keras.Sequential([\n",
    "    encoder_0,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_0.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_LSTM0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_LSTM0.fit(X_train_join[0], y_train, epochs=20,\n",
    "                    batch_size = 32,\n",
    "                    validation_data= (X_valid_join[0], y_valid),\n",
    "                    validation_steps=30\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_LSTM0.evaluate(X_test_join[0], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_LSTM0.predict(X_test_join[0])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM1 = tf.keras.Sequential([\n",
    "    encoder_1,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_1.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_LSTM1.fit(X_train_join[1], y_train, epochs=10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data= (X_valid_join[1], y_valid),\n",
    "                    validation_steps=30\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_LSTM1.evaluate(X_test_join[1], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_LSTM1.predict(X_test_join[1])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be up to 512 for BERT\n",
    "max_length = 256\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(texts, labels, limit=-1):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "  # for review, label in tfds.as_numpy(ds):\n",
    "  for text, label in zip(texts, labels):\n",
    "    bert_input = convert_example_to_feature(text)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    label_list.append([label])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_0 = encode_examples(X_train_join[0], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_0 = encode_examples(X_test_join[0], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_0 = encode_examples(X_valid_join[0], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_0 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_0.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_0.fit(ds_train_encoded_0, epochs=number_of_epochs, validation_data=ds_valid_encoded_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_0.predict(ds_test_encoded_0)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_1 = encode_examples(X_train_join[1], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_1 = encode_examples(X_test_join[1], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_1 = encode_examples(X_valid_join[1], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_1 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_1.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_1.fit(ds_train_encoded_1, epochs=number_of_epochs, validation_data=ds_valid_encoded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_1.predict(ds_test_encoded_1)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]]\n",
    "\n",
    "X_join = df_cluster[\"reviews_pipeline_0\"].apply(\" \".join)\n",
    "X_join = X_join.to_numpy()\n",
    "#df_cluster[\"reviews_join\"] = X_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "bow_vec = vectorizer.fit_transform(X_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "#words = vectorizer.get_feature_names_out()\n",
    "\n",
    "#setup kmeans clustering\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 420)\n",
    "#fit the data \n",
    "kmeans.fit(bow_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/k-means-clustering-chardonnay-reviews-using-scikit-learn-nltk-9df3c59527f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Palavras mais comuns em cada cluster\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando vector quantization (https://stackoverflow.com/questions/21660937/get-nearest-point-to-centroid-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids: N-dimensional array with your centroids\n",
    "# points:    N-dimensional array with your data points\n",
    "closest, distances = vq(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/39766593/get-element-closest-to-cluster-centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_k_closest(centroids, data, k=1, distance_norm=2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    ----------\n",
    "        centroids: (M, d) ndarray\n",
    "            M - number of clusters\n",
    "            d - number of data dimensions\n",
    "        data: (N, d) ndarray\n",
    "            N - number of data points\n",
    "        k: int (default 1)\n",
    "            nearest neighbour to get\n",
    "        distance_norm: int (default 2)\n",
    "            1: Hamming distance (x+y)\n",
    "            2: Euclidean distance (sqrt(x^2 + y^2))\n",
    "            np.inf: maximum distance in any dimension (max((x,y)))\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        indices: (M,) ndarray\n",
    "        values: (M, d) ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    kdtree = cKDTree(data)\n",
    "    distances, indices = kdtree.query(centroids, k, p=distance_norm)\n",
    "    if k > 1:\n",
    "        indices = indices[:,-1]\n",
    "    values = data[indices]\n",
    "    return indices, values\n",
    "\n",
    "indices, values = find_k_closest(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achar os n mais proximos (https://stackoverflow.com/questions/26795535/output-50-samples-closest-to-each-cluster-center-using-scikit-learn-k-means-libr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n Reviews mais próximas do centróide da classe 0 (Ruim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 0] #distancia de cada ponto ao centroide 0\n",
    "ind0 = np.argsort(d)[::][:n]\n",
    "ind0\n",
    "#csr_matrix.toarray(bow_vec)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind0[0]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind0[1]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_neg = []\n",
    "for i in ind0.tolist():\n",
    "    lista_neg.append(df_pp.iloc[i][\"reviews\"])\n",
    "lista_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n Reviews mais próximas do centróide da classe 1 (Bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 1] #distancia de cada ponto ao centroide 1\n",
    "ind1 = np.argsort(d)[::][:n]\n",
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind1[0]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind1[1]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_posi = []\n",
    "for i in ind1.tolist():\n",
    "    lista_posi.append(df_pp.iloc[i][\"reviews\"])\n",
    "lista_posi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando em um df para incluir no dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = pd.DataFrame(data={\"negativo\": lista_neg, \"positivo\": lista_posi})\n",
    "df_rep.to_csv(\"reviews_rep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para salvar predições do melhor classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[\"pred\"] = result\n",
    "#df_pred = pd.merge(X_test[[\"class\", \"pred\"]], df_pp[[\"reviews\", \"stars\", \"dates\"]], left_index=True, right_index=True)\n",
    "#df_pred.to_csv(\"best_pred.csv\", index=False)\n",
    "\n",
    "def save_results(result, X_test, df_pp, dfname=\"best_pred.csv\"):\n",
    "    X_test[\"pred\"] = result\n",
    "    df_pred = pd.merge(X_test[[\"class\", \"pred\"]], df_pp[[\"reviews\", \"stars\", \"dates\"]], left_index=True, right_index=True)\n",
    "    df_pred.to_csv(dfname, index=False)\n",
    "\n",
    "save_results(result, X_test, df_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"best_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
