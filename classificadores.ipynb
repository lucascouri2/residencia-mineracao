{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto - Mineração de Texto e Web\n",
    "## Residência Engenharia e Ciência de dados - Samsung/UFPE\n",
    "\n",
    "### Lucas Couri - lncc2\n",
    "### Mariama Oliveira - mcso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv2D, Input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataset_split \n",
    "import networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews_v2.csv\")\n",
    "df = df[df[\"reviews\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento (com e sem stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "other_punctuation = '—“”'  \n",
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.append('’')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "\n",
    "#Function that removes punctuation \n",
    "def remove_punctuation(text):\n",
    "    punctuation_free_doc = \"\".join([i for i in text if i not in string.punctuation+other_punctuation])\n",
    "    return punctuation_free_doc\n",
    "\n",
    "\n",
    "def remove_stopwords(list_words):\n",
    "    filtered_words = [word for word in list_words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def do_stemming(list_words):\n",
    "    stem_text = [stemmer.stem(word) for word in list_words]\n",
    "    return stem_text\n",
    "\n",
    "\n",
    "def pre_process(doc, basic_processing = False, no_stopwords = False, stemming = False):\n",
    "\n",
    "    final_doc = doc\n",
    "    \n",
    "    ## print(final_doc)\n",
    "\n",
    "    if basic_processing == True:\n",
    "        \n",
    "        final_doc = remove_punctuation(doc)\n",
    "        final_doc = final_doc.lower()\n",
    "\n",
    "    final_doc = nltk.word_tokenize(final_doc)\n",
    "\n",
    "    if no_stopwords == True:\n",
    "        final_doc = remove_stopwords(final_doc)    \n",
    "\n",
    "    if stemming == True:\n",
    "        final_doc = do_stemming(final_doc)\n",
    "\n",
    "    return final_doc\n",
    "\n",
    "def pre_process_all(df, pre_processing_list):\n",
    "\n",
    "    for param, index in zip(pre_processing_list, range(len(pre_processing_list))):\n",
    "        \n",
    "        df[f\"reviews_pipeline_{index}\"] = df[\"reviews\"].apply(lambda x: pre_process(x, **param))\n",
    "\n",
    "    return df\n",
    "\n",
    "pre_processing_list = [\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": False},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": True}]\n",
    "\n",
    "df_pp = pre_process_all(df, pre_processing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[\"class\"] = df_pp[\"stars\"].apply(lambda x : 1 if x >=4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão Train, Validation e Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unbalanced dataset\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = dataset_split.split_data(df_pp)\n",
    "\n",
    "#Balanced dataset\n",
    "X_train_b, X_valid_b, X_test_b, y_train_b, y_valid_b, y_test_b = dataset_split.split_data(df_pp, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando tokens em string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(df):\n",
    "    X_train_join = []\n",
    "    X_train_join.append(df[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "    X_train_join.append(df[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "    X_train_join[0] = X_train_join[0].to_numpy()\n",
    "    X_train_join[1] = X_train_join[1].to_numpy()\n",
    "\n",
    "    return X_train_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unbalanced data\n",
    "X_train_join = tokens_to_string(X_train)\n",
    "X_test_join = tokens_to_string(X_test)\n",
    "X_valid_join = tokens_to_string(X_valid)\n",
    "\n",
    "#Balanced data\n",
    "X_train_join_b = tokens_to_string(X_train_b)\n",
    "X_test_join_b = tokens_to_string(X_test_b)\n",
    "X_valid_join_b = tokens_to_string(X_valid_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest com BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "X_train_vec = []\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[0]))\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[1]))\n",
    "\n",
    "X_test_vec = []\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[0]))\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[1]))\n",
    "\n",
    "# X_valid_vec = []\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[0]))\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[1]))\n",
    "\n",
    "\n",
    "print(X_train_vec[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "def objective(trial):\n",
    "      iris = sklearn.datasets.load_iris()\n",
    "      n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
    "      max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))\n",
    "      clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "      return sklearn.model_selection.cross_val_score(clf, iris.data, iris.target, \n",
    "           n_jobs=-1, cv=3).mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_rf(X_train, y_train, parameters, cv, SEED):\n",
    "\n",
    "    rf = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "    search = GridSearchCV(rf,\n",
    "                          parameters,\n",
    "                          scoring = \"accuracy\",\n",
    "                          n_jobs = -1,\n",
    "                          cv = cv)\n",
    "\n",
    "    result_rf = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Grid Search para Random Forest==========')\n",
    "    print(f'Melhor Score: {result_rf.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_rf.best_params_}')\n",
    "\n",
    "    return result_rf\n",
    "\n",
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_0 = val_rf(X_train_vec[0], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_0.best_params_) \n",
    "forest = forest.fit(X_train_vec[0], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[0]) \n",
    "result = forest.predict(X_test_vec[0])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_1 = val_rf(X_train_vec[1], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_1.best_params_) \n",
    "forest = forest.fit(X_train_vec[1], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[1]) \n",
    "result = forest.predict(X_test_vec[1])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes (CNN, LSTM e BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_cnn_lstm(tuner_name,X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\n",
    "    type_nn = tuner_name[:3]\n",
    "   \n",
    "    #Encoding sentences\n",
    "    encoder = networks.sentence_encoder(X_train)\n",
    "\n",
    "    #Finding best parameters\n",
    "    tuner = networks.network_tuner(type_nn, encoder, tuner_name)\n",
    "    \n",
    "    best_hp = networks.search_network(tuner, X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    #Loading best model\n",
    "    model = networks.get_model(tuner, best_hp)\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = model.fit(X_train, y_train, epochs=50, validation_data = (X_valid, y_valid), callbacks=[stop_early])\n",
    "\n",
    "    #Getting test results\n",
    "    networks.get_test_metrics(model, X_test, y_test)\n",
    "    networks.plot_acuracy_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desbalanceado\n",
    "pipeline_cnn_lstm(\"cnn\",X_train_join[0], y_train, X_valid_join[0], y_valid, X_test_join[0], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceado\n",
    "pipeline_cnn_lstm(\"cnn_b\",X_train_join_b[0], y_train_b, X_valid_join_b[0], y_valid_b, X_test_join_b[0], y_test_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desbalanceado\n",
    "pipeline_cnn_lstm(\"cnn_st\",X_train_join[1], y_train, X_valid_join[1], y_valid, X_test_join[1], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanceado\n",
    "pipeline_cnn_lstm(\"cnn_b_st\",X_train_join_b[1], y_train_b, X_valid_join_b[1], y_valid_b, X_test_join_b[1], y_test_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cnn_lstm(\"lstm\",X_train_join[0], y_train, X_valid_join[0], y_valid, X_test_join[0], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_example_to_feature(review):\n",
    "#   return tokenizer.encode_plus(review,\n",
    "#                 add_special_tokens = True, # add [CLS], [SEP]\n",
    "#                 max_length = max_length, # max length of the text that can go to BERT\n",
    "#                 pad_to_max_length = True, # add [PAD] tokens\n",
    "#                 return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can be up to 512 for BERT\n",
    "# max_length = 256\n",
    "# batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "#   return {\n",
    "#       \"input_ids\": input_ids,\n",
    "#       \"token_type_ids\": token_type_ids,\n",
    "#       \"attention_mask\": attention_masks,\n",
    "#   }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(texts, labels, limit=-1):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "  # for review, label in tfds.as_numpy(ds):\n",
    "  for text, label in zip(texts, labels):\n",
    "    bert_input = convert_example_to_feature(text)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    label_list.append([label])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_0 = encode_examples(X_train_join[0], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_0 = encode_examples(X_test_join[0], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_0 = encode_examples(X_valid_join[0], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_0 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_0.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_0.fit(ds_train_encoded_0, epochs=number_of_epochs, validation_data=ds_valid_encoded_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_0.predict(ds_test_encoded_0)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_1 = encode_examples(X_train_join[1], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_1 = encode_examples(X_test_join[1], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_1 = encode_examples(X_valid_join[1], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_1 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_1.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_1.fit(ds_train_encoded_1, epochs=number_of_epochs, validation_data=ds_valid_encoded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_1.predict(ds_test_encoded_1)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]]\n",
    "\n",
    "X_join = df_cluster[\"reviews_pipeline_0\"].apply(\" \".join)\n",
    "X_join = X_join.to_numpy()\n",
    "#df_cluster[\"reviews_join\"] = X_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "bow_vec = vectorizer.fit_transform(X_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "#words = vectorizer.get_feature_names_out()\n",
    "\n",
    "#setup kmeans clustering\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 420)\n",
    "#fit the data \n",
    "kmeans.fit(bow_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/k-means-clustering-chardonnay-reviews-using-scikit-learn-nltk-9df3c59527f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Palavras mais comuns em cada cluster\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando vector quantization (https://stackoverflow.com/questions/21660937/get-nearest-point-to-centroid-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids: N-dimensional array with your centroids\n",
    "# points:    N-dimensional array with your data points\n",
    "closest, distances = vq(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/39766593/get-element-closest-to-cluster-centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_k_closest(centroids, data, k=1, distance_norm=2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    ----------\n",
    "        centroids: (M, d) ndarray\n",
    "            M - number of clusters\n",
    "            d - number of data dimensions\n",
    "        data: (N, d) ndarray\n",
    "            N - number of data points\n",
    "        k: int (default 1)\n",
    "            nearest neighbour to get\n",
    "        distance_norm: int (default 2)\n",
    "            1: Hamming distance (x+y)\n",
    "            2: Euclidean distance (sqrt(x^2 + y^2))\n",
    "            np.inf: maximum distance in any dimension (max((x,y)))\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        indices: (M,) ndarray\n",
    "        values: (M, d) ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    kdtree = cKDTree(data)\n",
    "    distances, indices = kdtree.query(centroids, k, p=distance_norm)\n",
    "    if k > 1:\n",
    "        indices = indices[:,-1]\n",
    "    values = data[indices]\n",
    "    return indices, values\n",
    "\n",
    "indices, values = find_k_closest(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achar os n mais proximos (https://stackoverflow.com/questions/26795535/output-50-samples-closest-to-each-cluster-center-using-scikit-learn-k-means-libr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n Reviews mais próximas do centróide da classe 0 (Ruim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 0] #distancia de cada ponto ao centroide 0\n",
    "ind0 = np.argsort(d)[::][:n]\n",
    "ind0\n",
    "#csr_matrix.toarray(bow_vec)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind0[0]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind0[1]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_neg = []\n",
    "for i in ind0.tolist():\n",
    "    lista_neg.append(df_pp.iloc[i][\"reviews\"])\n",
    "lista_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n Reviews mais próximas do centróide da classe 1 (Bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 1] #distancia de cada ponto ao centroide 1\n",
    "ind1 = np.argsort(d)[::][:n]\n",
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind1[0]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp.iloc[ind1[1]][\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_posi = []\n",
    "for i in ind1.tolist():\n",
    "    lista_posi.append(df_pp.iloc[i][\"reviews\"])\n",
    "lista_posi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando em um df para incluir no dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rep = pd.DataFrame(data={\"negativo\": lista_neg, \"positivo\": lista_posi})\n",
    "df_rep.to_csv(\"reviews_rep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para salvar predições do melhor classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[\"pred\"] = result\n",
    "#df_pred = pd.merge(X_test[[\"class\", \"pred\"]], df_pp[[\"reviews\", \"stars\", \"dates\"]], left_index=True, right_index=True)\n",
    "#df_pred.to_csv(\"best_pred.csv\", index=False)\n",
    "\n",
    "def save_results(result, X_test, df_pp, dfname=\"best_pred.csv\"):\n",
    "    X_test[\"pred\"] = result\n",
    "    df_pred = pd.merge(X_test[[\"class\", \"pred\"]], df_pp[[\"reviews\", \"stars\", \"dates\"]], left_index=True, right_index=True)\n",
    "    df_pred.to_csv(dfname, index=False)\n",
    "\n",
    "save_results(result, X_test, df_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"best_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
