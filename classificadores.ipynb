{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto - Mineração de Texto e Web\n",
    "## Residência Engenharia e Ciência de dados - Samsung/UFPE\n",
    "\n",
    "### Lucas Couri - lncc2\n",
    "### Mariama Oliveira - mcso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv2D, Input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews_v2.csv\")\n",
    "df = df[df[\"reviews\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews    object\n",
       "stars       int64\n",
       "dates      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>stars</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou aqui para relatar uma experiência que ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-06-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já havia comprado a versão Lite, o que já ac...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bom. Bem fluído, interessante e eficaz em cu...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou bem chateado, ja possuía o modelo anti...</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aparelho muito bom, está lidando muito bem c...</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-05-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  stars       dates\n",
       "0    Estou aqui para relatar uma experiência que ...      5  2021-06-05\n",
       "1    Já havia comprado a versão Lite, o que já ac...      5  2021-05-07\n",
       "2    Bom. Bem fluído, interessante e eficaz em cu...      5  2021-05-07\n",
       "3    Estou bem chateado, ja possuía o modelo anti...      3  2021-05-09\n",
       "4    Aparelho muito bom, está lidando muito bem c...      4  2021-05-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento (com e sem stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "other_punctuation = '—“”'  \n",
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.append('’')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "\n",
    "#Function that removes punctuation \n",
    "def remove_punctuation(text):\n",
    "    punctuation_free_doc = \"\".join([i for i in text if i not in string.punctuation+other_punctuation])\n",
    "    return punctuation_free_doc\n",
    "\n",
    "\n",
    "def remove_stopwords(list_words):\n",
    "    filtered_words = [word for word in list_words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def do_stemming(list_words):\n",
    "    stem_text = [stemmer.stem(word) for word in list_words]\n",
    "    return stem_text\n",
    "\n",
    "\n",
    "def pre_process(doc, basic_processing = False, no_stopwords = False, stemming = False):\n",
    "\n",
    "    final_doc = doc\n",
    "    \n",
    "    ## print(final_doc)\n",
    "\n",
    "    if basic_processing == True:\n",
    "        \n",
    "        final_doc = remove_punctuation(doc)\n",
    "        final_doc = final_doc.lower()\n",
    "\n",
    "    final_doc = nltk.word_tokenize(final_doc)\n",
    "\n",
    "    if no_stopwords == True:\n",
    "        final_doc = remove_stopwords(final_doc)    \n",
    "\n",
    "    if stemming == True:\n",
    "        final_doc = do_stemming(final_doc)\n",
    "\n",
    "    return final_doc\n",
    "\n",
    "def pre_process_all(df, pre_processing_list):\n",
    "\n",
    "    for param, index in zip(pre_processing_list, range(len(pre_processing_list))):\n",
    "        \n",
    "        df[f\"reviews_pipeline_{index}\"] = df[\"reviews\"].apply(lambda x: pre_process(x, **param))\n",
    "\n",
    "    return df\n",
    "\n",
    "pre_processing_list = [\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": False},\n",
    "    {\"basic_processing\": True, \"no_stopwords\": True, \"stemming\": True}]\n",
    "\n",
    "df_pp = pre_process_all(df, pre_processing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>stars</th>\n",
       "      <th>dates</th>\n",
       "      <th>reviews_pipeline_0</th>\n",
       "      <th>reviews_pipeline_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou aqui para relatar uma experiência que ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>[aqui, relatar, experiência, visando, contribu...</td>\n",
       "      <td>[aqu, relat, experi, vis, contribu, amig, prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já havia comprado a versão Lite, o que já ac...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-07</td>\n",
       "      <td>[havia, comprado, versão, lite, achei, maravil...</td>\n",
       "      <td>[hav, compr, vers, lit, ach, maravilh, porém, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bom. Bem fluído, interessante e eficaz em cu...</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-07</td>\n",
       "      <td>[bom, bem, fluído, interessante, eficaz, cumpr...</td>\n",
       "      <td>[bom, bem, flu, interess, eficaz, cumpr, prome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou bem chateado, ja possuía o modelo anti...</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-05-09</td>\n",
       "      <td>[bem, chateado, ja, possuía, modelo, antigo, b...</td>\n",
       "      <td>[bem, chate, ja, possuí, model, antig, bem, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aparelho muito bom, está lidando muito bem c...</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>[aparelho, bom, lidando, bem, home, tv, apenas...</td>\n",
       "      <td>[aparelh, bom, lid, bem, hom, tv, apen, porém,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  stars       dates  \\\n",
       "0    Estou aqui para relatar uma experiência que ...      5  2021-06-05   \n",
       "1    Já havia comprado a versão Lite, o que já ac...      5  2021-05-07   \n",
       "2    Bom. Bem fluído, interessante e eficaz em cu...      5  2021-05-07   \n",
       "3    Estou bem chateado, ja possuía o modelo anti...      3  2021-05-09   \n",
       "4    Aparelho muito bom, está lidando muito bem c...      4  2021-05-06   \n",
       "\n",
       "                                  reviews_pipeline_0  \\\n",
       "0  [aqui, relatar, experiência, visando, contribu...   \n",
       "1  [havia, comprado, versão, lite, achei, maravil...   \n",
       "2  [bom, bem, fluído, interessante, eficaz, cumpr...   \n",
       "3  [bem, chateado, ja, possuía, modelo, antigo, b...   \n",
       "4  [aparelho, bom, lidando, bem, home, tv, apenas...   \n",
       "\n",
       "                                  reviews_pipeline_1  \n",
       "0  [aqu, relat, experi, vis, contribu, amig, prob...  \n",
       "1  [hav, compr, vers, lit, ach, maravilh, porém, ...  \n",
       "2  [bom, bem, flu, interess, eficaz, cumpr, prome...  \n",
       "3  [bem, chate, ja, possuí, model, antig, bem, co...  \n",
       "4  [aparelh, bom, lid, bem, hom, tv, apen, porém,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_pipeline_0</th>\n",
       "      <th>reviews_pipeline_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aqui, relatar, experiência, visando, contribu...</td>\n",
       "      <td>[aqu, relat, experi, vis, contribu, amig, prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[havia, comprado, versão, lite, achei, maravil...</td>\n",
       "      <td>[hav, compr, vers, lit, ach, maravilh, porém, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bom, bem, fluído, interessante, eficaz, cumpr...</td>\n",
       "      <td>[bom, bem, flu, interess, eficaz, cumpr, prome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bem, chateado, ja, possuía, modelo, antigo, b...</td>\n",
       "      <td>[bem, chate, ja, possuí, model, antig, bem, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[aparelho, bom, lidando, bem, home, tv, apenas...</td>\n",
       "      <td>[aparelh, bom, lid, bem, hom, tv, apen, porém,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>[chegou, super, rápido, atendeu, superou, toda...</td>\n",
       "      <td>[cheg, sup, rápid, atend, super, tod, expect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>[facil, instalação, configuração, entrega, sup...</td>\n",
       "      <td>[facil, instal, configur, entreg, sup, rápid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>[amei, produto, unico, problema, pra, mim, nao...</td>\n",
       "      <td>[ame, produt, unic, problem, pra, mim, nao, hbo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>[funciona, beleza, rede, internet, sendo, boa,...</td>\n",
       "      <td>[func, bel, red, internet, send, boa, tud, bem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>[aparelho, bom, penanao, poder, instalar, hbo,...</td>\n",
       "      <td>[aparelh, bom, penana, pod, instal, hbo, max]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5003 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     reviews_pipeline_0  \\\n",
       "0     [aqui, relatar, experiência, visando, contribu...   \n",
       "1     [havia, comprado, versão, lite, achei, maravil...   \n",
       "2     [bom, bem, fluído, interessante, eficaz, cumpr...   \n",
       "3     [bem, chateado, ja, possuía, modelo, antigo, b...   \n",
       "4     [aparelho, bom, lidando, bem, home, tv, apenas...   \n",
       "...                                                 ...   \n",
       "5002  [chegou, super, rápido, atendeu, superou, toda...   \n",
       "5003  [facil, instalação, configuração, entrega, sup...   \n",
       "5004  [amei, produto, unico, problema, pra, mim, nao...   \n",
       "5005  [funciona, beleza, rede, internet, sendo, boa,...   \n",
       "5006  [aparelho, bom, penanao, poder, instalar, hbo,...   \n",
       "\n",
       "                                     reviews_pipeline_1  \n",
       "0     [aqu, relat, experi, vis, contribu, amig, prob...  \n",
       "1     [hav, compr, vers, lit, ach, maravilh, porém, ...  \n",
       "2     [bom, bem, flu, interess, eficaz, cumpr, prome...  \n",
       "3     [bem, chate, ja, possuí, model, antig, bem, co...  \n",
       "4     [aparelh, bom, lid, bem, hom, tv, apen, porém,...  \n",
       "...                                                 ...  \n",
       "5002      [cheg, sup, rápid, atend, super, tod, expect]  \n",
       "5003      [facil, instal, configur, entreg, sup, rápid]  \n",
       "5004   [ame, produt, unic, problem, pra, mim, nao, hbo]  \n",
       "5005    [func, bel, red, internet, send, boa, tud, bem]  \n",
       "5006      [aparelh, bom, penana, pod, instal, hbo, max]  \n",
       "\n",
       "[5003 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp[[\"reviews_pipeline_0\", \"reviews_pipeline_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp[\"class\"] = df_pp[\"stars\"].apply(lambda x : 1 if x >=4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pp[[\"reviews_pipeline_0\", \"reviews_pipeline_1\"]]\n",
    "y = df_pp[\"class\"]\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.3, random_state = 42, stratify=y_train_valid)\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_pipeline_0</th>\n",
       "      <th>reviews_pipeline_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>[fácil, acesso, mto, bom, porém, comprei, vers...</td>\n",
       "      <td>[fácil, acess, mto, bom, porém, compr, vers, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>[prática, tv, inteligente, completa, melhor]</td>\n",
       "      <td>[prát, tv, intelig, complet, melhor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>[produto, funciona, bem, faltou, controle, par...</td>\n",
       "      <td>[produt, func, bem, falt, control, parametr, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>[produto, perfeito, medo, comprar, pq, dissera...</td>\n",
       "      <td>[produt, perfeit, med, compr, pq, diss, q, pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>[aplicativo, jr, play, funciona, tentar, baixa...</td>\n",
       "      <td>[aplic, jr, play, func, tent, baix, instal, ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     reviews_pipeline_0  \\\n",
       "877   [fácil, acesso, mto, bom, porém, comprei, vers...   \n",
       "3525       [prática, tv, inteligente, completa, melhor]   \n",
       "3941  [produto, funciona, bem, faltou, controle, par...   \n",
       "1468  [produto, perfeito, medo, comprar, pq, dissera...   \n",
       "819   [aplicativo, jr, play, funciona, tentar, baixa...   \n",
       "\n",
       "                                     reviews_pipeline_1  \n",
       "877   [fácil, acess, mto, bom, porém, compr, vers, c...  \n",
       "3525               [prát, tv, intelig, complet, melhor]  \n",
       "3941  [produt, func, bem, falt, control, parametr, t...  \n",
       "1468  [produt, perfeit, med, compr, pq, diss, q, pou...  \n",
       "819   [aplic, jr, play, func, tent, baix, instal, ap...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando tokens em string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_join = []\n",
    "X_train_join.append(X_train[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_train_join.append(X_train[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_train_join[0] = X_train_join[0].to_numpy()\n",
    "X_train_join[1] = X_train_join[1].to_numpy()\n",
    "\n",
    "X_test_join = []\n",
    "X_test_join.append(X_test[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_test_join.append(X_test[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_test_join[0] = X_test_join[0].to_numpy()\n",
    "X_test_join[1] = X_test_join[1].to_numpy()\n",
    "\n",
    "X_valid_join = []\n",
    "X_valid_join.append(X_valid[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_valid_join.append(X_valid[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_valid_join[0] = X_valid_join[0].to_numpy()\n",
    "X_valid_join[1] = X_valid_join[1].to_numpy()\n",
    "\n",
    "X_train_valid_join = []\n",
    "X_train_valid_join.append(X_train_valid[\"reviews_pipeline_0\"].apply(\" \".join))\n",
    "X_train_valid_join.append(X_train_valid[\"reviews_pipeline_1\"].apply(\" \".join))\n",
    "X_train_valid_join[0] = X_train_valid_join[0].to_numpy()\n",
    "X_train_valid_join[1] = X_train_valid_join[1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest com BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "X_train_vec = []\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[0]))\n",
    "X_train_vec.append(vectorizer.fit_transform(X_train_valid_join[1]))\n",
    "\n",
    "X_test_vec = []\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[0]))\n",
    "X_test_vec.append(vectorizer.fit_transform(X_test_join[1]))\n",
    "\n",
    "# X_valid_vec = []\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[0]))\n",
    "# X_valid_vec.append(vectorizer.fit_transform(X_valid_join[1]))\n",
    "\n",
    "\n",
    "print(X_train_vec[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_rf(X_train, y_train, parameters, cv, SEED):\n",
    "\n",
    "    rf = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "    search = GridSearchCV(rf,\n",
    "                          parameters,\n",
    "                          scoring = \"accuracy\",\n",
    "                          n_jobs = -1,\n",
    "                          cv = cv)\n",
    "\n",
    "    result_rf = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Grid Search para Random Forest==========')\n",
    "    print(f'Melhor Score: {result_rf.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_rf.best_params_}')\n",
    "\n",
    "    return result_rf\n",
    "\n",
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_0 = val_rf(X_train_vec[0], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_0.best_params_) \n",
    "forest = forest.fit(X_train_vec[0], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[0]) \n",
    "result = forest.predict(X_test_vec[0])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 101, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "#parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "#parameters['min_samples_leaf'] = [1, 4]\n",
    "#parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "result_rf_1 = val_rf(X_train_vec[1], y_train_valid, parameters, cv, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(**result_rf_1.best_params_) \n",
    "forest = forest.fit(X_train_vec[1], y_train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forest.predict(X_test_vec[1]) \n",
    "result = forest.predict(X_test_vec[1])\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes (CNN, LSTM e BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=None,\n",
    "#     standardize='lower_and_strip_punctuation',\n",
    "#     split='whitespace',\n",
    "#     ngrams=None,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=None,\n",
    "#     pad_to_max_tokens=False,\n",
    "#     vocabulary=None,\n",
    "#     idf_weights=None,\n",
    "#     sparse=False,\n",
    "#     ragged=False,\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_join[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Sem stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder_0 = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                            standardize=None\n",
    "                                            )\n",
    "#encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "encoder_0.adapt(X_train_join[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = encoder_0(X_train_join[0])\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder_0.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder (Com stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder_1 = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                            standardize=None\n",
    "                                            )\n",
    "#encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "encoder_1.adapt(X_train_join[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = encoder_1(X_train_join[1])\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder_1.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "##Testando keras tuner\n",
    "\n",
    "def model_builder_0(hp):\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    model_CNN_0 = tf.keras.Sequential([\n",
    "    encoder_0,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_0.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    #tf.keras.layers.Flatten(),    \n",
    "    tf.keras.layers.Dense(units=hp_units, activation='relu'), #>>>>>Hiperparametro\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model_CNN_0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), #>>>>>Hiperparametro\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_CNN_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_0 = tf.keras.Sequential([\n",
    "#     encoder_0,\n",
    "#     tf.keras.layers.Embedding(\n",
    "#         input_dim=len(encoder_0.get_vocabulary()),\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True),\n",
    "#     tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "#     tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "#     #tf.keras.layers.Flatten(),    \n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.GlobalMaxPool1D(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_CNN_0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_CNN_0.fit(X_train_join[0], y_train, epochs=30,\n",
    "#                     batch_size = 32,\n",
    "#                     validation_data= (X_valid_join[0], y_valid),\n",
    "#                     validation_steps=30\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder_0, # the hypermodel\n",
    "                     objective='val_accuracy', # objective to optimize\n",
    "max_epochs=10,\n",
    "factor=3, # factor which you have seen above \n",
    "directory='tuner', # directory to save logs \n",
    "project_name='cnn_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning settings\n",
    "tuner.search_space_summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Perform hypertuning\n",
    "tuner.search(X_train_join[1], y_train, epochs=10, validation_data = (X_valid_join[0], y_valid), callbacks=[stop_early])\n",
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "model_CNN_0 = tuner.hypermodel.build(best_hp)\n",
    "model_CNN_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_CNN_0.fit(X_train_join[0], y_train, epochs=10, validation_data = (X_valid_join[0], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_CNN_0.evaluate(X_test_join[0], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_CNN_0.predict(X_test_join[0])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_1 = tf.keras.Sequential([\n",
    "#     encoder_1,\n",
    "#     tf.keras.layers.Embedding(\n",
    "#         input_dim=len(encoder_1.get_vocabulary()),\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True),\n",
    "#     tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "#     tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "#     #tf.keras.layers.Flatten(),    \n",
    "#     tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.GlobalMaxPool1D(),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "##Testando keras tuner\n",
    "\n",
    "def model_builder_1(hp):\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "    model_CNN_1 = tf.keras.Sequential([\n",
    "    encoder_1,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_1.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "    #tf.keras.layers.Flatten(),    \n",
    "    tf.keras.layers.Dense(units=hp_units, activation='relu'), #>>>>>Hiperparametro\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model_CNN_1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), #>>>>>Hiperparametro\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model_CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder_1, # the hypermodel\n",
    "                     objective='val_accuracy', # objective to optimize\n",
    "max_epochs=10,\n",
    "factor=3, # factor which you have seen above \n",
    "directory='tuner', # directory to save logs \n",
    "project_name='cnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning settings\n",
    "tuner.search_space_summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Perform hypertuning\n",
    "tuner.search(X_train_join[1], y_train, epochs=10, validation_data = (X_valid_join[1], y_valid), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "model_CNN_1 = tuner.hypermodel.build(best_hp)\n",
    "model_CNN_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_CNN_1.fit(X_train_join[1], y_train, epochs=10, validation_data = (X_valid_join[1], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_CNN_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_CNN_1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_CNN_1.fit(X_train_join[1], y_train, epochs=20,\n",
    "#                     batch_size = 32,\n",
    "#                     validation_data= (X_valid_join[1], y_valid),\n",
    "#                     validation_steps=3\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model_CNN_1.evaluate(X_test_join[1], y_test)\n",
    "test_loss, test_acc = model_CNN_1.evaluate(X_test_join[1], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = h_model.predict(X_test_join[1])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM0 = tf.keras.Sequential([\n",
    "    encoder_0,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_0.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM0.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_LSTM0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_LSTM0.fit(X_train_join[0], y_train, epochs=20,\n",
    "                    batch_size = 32,\n",
    "                    validation_data= (X_valid_join[0], y_valid),\n",
    "                    validation_steps=30\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_LSTM0.evaluate(X_test_join[0], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_LSTM0.predict(X_test_join[0])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM1 = tf.keras.Sequential([\n",
    "    encoder_1,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder_1.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM1.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_LSTM1.fit(X_train_join[1], y_train, epochs=10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data= (X_valid_join[1], y_valid),\n",
    "                    validation_steps=30\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_LSTM1.evaluate(X_test_join[1], y_test)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_LSTM1.predict(X_test_join[1])\n",
    "result = np.where(result > 0.5, 1, 0)\n",
    "result\n",
    "\n",
    "print(classification_report(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be up to 512 for BERT\n",
    "max_length = 256\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(texts, labels, limit=-1):\n",
    "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "  # for review, label in tfds.as_numpy(ds):\n",
    "  for text, label in zip(texts, labels):\n",
    "    bert_input = convert_example_to_feature(text)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "    label_list.append([label])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sem stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_0 = encode_examples(X_train_join[0], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_0 = encode_examples(X_test_join[0], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_0 = encode_examples(X_valid_join[0], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_0 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_0.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_0.fit(ds_train_encoded_0, epochs=number_of_epochs, validation_data=ds_valid_encoded_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_0.predict(ds_test_encoded_0)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded_1 = encode_examples(X_train_join[1], y_train).shuffle(3).batch(batch_size)\n",
    "\n",
    "# test dataset\n",
    "ds_test_encoded_1 = encode_examples(X_test_join[1], y_test).batch(batch_size)\n",
    "\n",
    "#validation dataset\n",
    "ds_valid_encoded_1 = encode_examples(X_valid_join[1], y_valid).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 3\n",
    "# model initialization\n",
    "model_bert_1 = TFBertForSequenceClassification.from_pretrained('bert-base-portuguese-cased', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_bert_1.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_history = model_bert_1.fit(ds_train_encoded_1, epochs=number_of_epochs, validation_data=ds_valid_encoded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output = model_bert_1.predict(ds_test_encoded_1)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label_pred = label.numpy()\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, label_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviews_pipeline_0</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estou aqui para relatar uma experiência que ...</td>\n",
       "      <td>[aqui, relatar, experiência, visando, contribu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já havia comprado a versão Lite, o que já ac...</td>\n",
       "      <td>[havia, comprado, versão, lite, achei, maravil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bom. Bem fluído, interessante e eficaz em cu...</td>\n",
       "      <td>[bom, bem, fluído, interessante, eficaz, cumpr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou bem chateado, ja possuía o modelo anti...</td>\n",
       "      <td>[bem, chateado, ja, possuía, modelo, antigo, b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aparelho muito bom, está lidando muito bem c...</td>\n",
       "      <td>[aparelho, bom, lidando, bem, home, tv, apenas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    Estou aqui para relatar uma experiência que ...   \n",
       "1    Já havia comprado a versão Lite, o que já ac...   \n",
       "2    Bom. Bem fluído, interessante e eficaz em cu...   \n",
       "3    Estou bem chateado, ja possuía o modelo anti...   \n",
       "4    Aparelho muito bom, está lidando muito bem c...   \n",
       "\n",
       "                                  reviews_pipeline_0  class  \n",
       "0  [aqui, relatar, experiência, visando, contribu...      1  \n",
       "1  [havia, comprado, versão, lite, achei, maravil...      1  \n",
       "2  [bom, bem, fluído, interessante, eficaz, cumpr...      1  \n",
       "3  [bem, chateado, ja, possuía, modelo, antigo, b...      0  \n",
       "4  [aparelho, bom, lidando, bem, home, tv, apenas...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_pp[[\"reviews\", \"reviews_pipeline_0\", \"class\"]]\n",
    "\n",
    "X_join = df_cluster[\"reviews_pipeline_0\"].apply(\" \".join)\n",
    "X_join = X_join.to_numpy()\n",
    "#df_cluster[\"reviews_join\"] = X_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2000) \n",
    "\n",
    "#List with BoWs (pipeline 0 and 1)\n",
    "bow_vec = vectorizer.fit_transform(X_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=420)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "#words = vectorizer.get_feature_names_out()\n",
    "\n",
    "#setup kmeans clustering\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 420)\n",
    "#fit the data \n",
    "kmeans.fit(bow_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02751032, 0.01788171, 0.00412655, ..., 0.0522696 , 0.00137552,\n",
       "        0.0261348 ],\n",
       "       [0.01005613, 0.01005613, 0.00140318, ..., 0.02923293, 0.00140318,\n",
       "        0.00841908]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/k-means-clustering-chardonnay-reviews-using-scikit-learn-nltk-9df3c59527f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : tv, controle, fire, stick, smart, bem, alexa, fácil, produto, volume\n",
      "1 : produto, fácil, tv, controle, bom, instalar, bem, alexa, aparelho, hbo\n"
     ]
    }
   ],
   "source": [
    "### Palavras mais comuns em cada cluster\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3650, 2336], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando vector quantization (https://stackoverflow.com/questions/21660937/get-nearest-point-to-centroid-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3650, 2336])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# centroids: N-dimensional array with your centroids\n",
    "# points:    N-dimensional array with your data points\n",
    "closest, distances = vq(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))\n",
    "closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/39766593/get-element-closest-to-cluster-centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def find_k_closest(centroids, data, k=1, distance_norm=2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    ----------\n",
    "        centroids: (M, d) ndarray\n",
    "            M - number of clusters\n",
    "            d - number of data dimensions\n",
    "        data: (N, d) ndarray\n",
    "            N - number of data points\n",
    "        k: int (default 1)\n",
    "            nearest neighbour to get\n",
    "        distance_norm: int (default 2)\n",
    "            1: Hamming distance (x+y)\n",
    "            2: Euclidean distance (sqrt(x^2 + y^2))\n",
    "            np.inf: maximum distance in any dimension (max((x,y)))\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        indices: (M,) ndarray\n",
    "        values: (M, d) ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    kdtree = cKDTree(data)\n",
    "    distances, indices = kdtree.query(centroids, k, p=distance_norm)\n",
    "    if k > 1:\n",
    "        indices = indices[:,-1]\n",
    "    values = data[indices]\n",
    "    return indices, values\n",
    "\n",
    "indices, values = find_k_closest(kmeans.cluster_centers_, csr_matrix.toarray(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3650, 4972], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achar os n mais proximos (https://stackoverflow.com/questions/26795535/output-50-samples-closest-to-each-cluster-center-using-scikit-learn-k-means-libr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 Reviews mais próximas do centróide da classe 0 (Ruim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3650, 4823, 4102, 4634, 4258], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 0] #distancia de cada ponto ao centroide 0\n",
    "ind0 = np.argsort(d)[::][:n]\n",
    "ind0\n",
    "#csr_matrix.toarray(bow_vec)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews                 Minha tv é compatível só que o controle não ...\n",
       "stars                                                                 1\n",
       "dates                                                        2021-05-11\n",
       "reviews_pipeline_0          [tv, compatível, controle, controlando, tv]\n",
       "reviews_pipeline_1                   [tv, compat, control, control, tv]\n",
       "class                                                                 0\n",
       "Name: 3651, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.iloc[3650]#[\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews                 Não consigo controlar o volume da tv com o f...\n",
       "stars                                                                 5\n",
       "dates                                                        2021-09-19\n",
       "reviews_pipeline_0           [consigo, controlar, volume, tv, fire, tv]\n",
       "reviews_pipeline_1                [consig, control, volum, tv, fir, tv]\n",
       "class                                                                 1\n",
       "Name: 4827, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.iloc[4823]#[\"reviews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 Reviews mais próximas do centróide da classe 1 (Bom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3603, 3217, 3998, 2336, 4972], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "d = kmeans.transform(csr_matrix.toarray(bow_vec))[:, 1] #distancia de cada ponto ao centroide 1\n",
    "ind1 = np.argsort(d)[::][:n]\n",
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews                 Adoreiiiiiiiii\n",
       "stars                                5\n",
       "dates                       2021-06-25\n",
       "reviews_pipeline_0    [adoreiiiiiiiii]\n",
       "reviews_pipeline_1     [adoreiiiiiiii]\n",
       "class                                1\n",
       "Name: 3604, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.iloc[3603]#[\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews                 Sensacional...recomendo.\n",
       "stars                                          5\n",
       "dates                                 2021-06-07\n",
       "reviews_pipeline_0        [sensacionalrecomendo]\n",
       "reviews_pipeline_1            [sensacionalrecom]\n",
       "class                                          1\n",
       "Name: 3217, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.iloc[3217]#[\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews                 Gosteiiii muito\n",
       "stars                                 5\n",
       "dates                        2022-01-17\n",
       "reviews_pipeline_0          [gosteiiii]\n",
       "reviews_pipeline_1           [gosteiii]\n",
       "class                                 1\n",
       "Name: 4000, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.iloc[3998]#[\"reviews\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
